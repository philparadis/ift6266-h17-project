{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar 15 14:54:01 2017\n",
    "\n",
    "@author: paradiph\n",
    "\"\"\"\n",
    "\n",
    "import os, sys\n",
    "import glob\n",
    "import cPickle as pkl\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "#from skimage.transform import resize\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras.utils import plot_model\n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# Run experiments here\n",
    "# Define your global options and experiment name\n",
    "# Then run the desired model\n",
    "#################################################\n",
    "\n",
    "### The experiment name is very important.\n",
    "\n",
    "## Your model will be saved in:                           models/<experiment_name>.h5\n",
    "## A summary of your model architecture will saved be in: models/summary_<experiment_name>.txt\n",
    "## Your model's performance will be saved in:             models/performance_<experiment_name>.txt\n",
    "\n",
    "## Your predictions will be saved in: predictions/assets/<experiment_name>/Y_pred_<i>.jpg\n",
    "##                                    predictions/assets/<experiment_name>/Y_<i>.jpg\n",
    "##                                    predictions/assets/<experiment_name>/X_outer_<i>.jpg\n",
    "##                                    predictions/assets/<experiment_name>/X_full_<i>.jpg\n",
    "##                                    predictions/assets/<experiment_name>/X_full_pred_<i>.jpg\n",
    "\n",
    "experiment_name = \"exp1_mlp_msa_nodropout\"\n",
    "#experiment_name = \"exp2_mlp_msa_nodropout\"\n",
    "#experiment_name = \"exp3_mlp_mse_sigmoid_final_layer\"\n",
    "#TODO: Which ever first 3 experiments work best, repeat it with msa instead of mse. i.e. experiment_name = \"exp4_mlp_msa_sigmoid_final_layer\"\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "loss_function = 'msa'\n",
    "use_dropout = False\n",
    "use_sigmoid_final_layer = False\n",
    "\n",
    "### Fixed variables: DO NOT CHANGE THOSE\n",
    "input_dim = 64*64*3 - 32*32*3\n",
    "output_dim = 32*32*3\n",
    "path_mscoco=\"datasets/mscoco_inpainting/inpainting/\"\n",
    "path_traindata=\"train2014\"\n",
    "path_caption_dict=\"dict_key_imgID_value_caps_train_and_valid.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### State variables: DO NOT EDIT\n",
    "### ONLY RUN THIS CELL IF YOU WANNA RESET EVERYTHING AND RELOAD THE DATA, RETRAIN THE MODEL, ETC.\n",
    "\n",
    "is_dataset_loaded = False\n",
    "is_model_trained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Info about the dataset\n",
    "#######################################\n",
    "# The data is already split into training and validation datasets\n",
    "# The training dataset has:\n",
    "# - 82782 items\n",
    "# - 984 MB of data\n",
    "# The validation dataset has:\n",
    "# - 40504 items\n",
    "# - 481 MB of data\n",
    "#\n",
    "# There is also a pickled dictionary that maps image filenames (minutes the\n",
    "# .jpg extension) to a list of 5 strings (the 5 human-generated captions).\n",
    "# This dictionary is an OrderedDict with 123286 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Utilities functions\n",
    "\n",
    "## Your model will be saved in:                           models/<experiment_name>.h5\n",
    "## A summary of your model architecture will saved be in: models/summary_<experiment_name>.txt\n",
    "## Your model's performance will be saved in:             models/performance_<experiment_name>.txt\n",
    "def save_model_info(exp_name, model):\n",
    "    out_dir = \"models/\"\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "        \n",
    "    model.save(os.path.join(out_dir, exp_name + '.h5')) \n",
    "    \n",
    "    #TODO: INSTALL pydot\n",
    "    #plot_model(model, to_file=os.path.join('model/', 'architecture_' + exp_name + '.png'), show_shapes=True)\n",
    "    \n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = open(os.path.join(out_dir, 'summary_' + exp_name + '.txt'), 'w')\n",
    "    model.summary()\n",
    "    sys.stdout = old_stdout\n",
    "\n",
    "    with open(os.path.join(out_dir, 'performance_' + exp_name + '.txt'), 'w') as fd:\n",
    "        # evaluate the model\n",
    "        scores = model.evaluate(X_train, Y_train, batch_size=batch_size)\n",
    "        fd.write(\"Training score %s: %.4f\\n\" % (model.metrics_names[1], scores[1]))\n",
    "        scores = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "        fd.write(\"Testing score %s: %.4f\\n\" % (model.metrics_names[1], scores[1]))\n",
    "        \n",
    "## Your predictions will be saved in: predictions/assets/<experiment_name>/Y_pred_<i>.jpg\n",
    "##                                    predictions/assets/<experiment_name>/Y_<i>.jpg\n",
    "##                                    predictions/assets/<experiment_name>/X_outer_<i>.jpg\n",
    "##                                    predictions/assets/<experiment_name>/X_full_<i>.jpg\n",
    "##                                    predictions/assets/<experiment_name>/X_full_pred_<i>.jpg\n",
    "def save_predictions_info(exp_name, pred, pred_indices, dataset,\n",
    "                          num_images = 10, show_images = False, use_flattened_datasets = True):\n",
    "    if use_flattened_datasets:\n",
    "        out_dir = os.path.join('predictions/', \"assets/\", exp_name)\n",
    "        if not os.path.exists(out_dir):\n",
    "            print(\"Creating new directory to save predictions results: \" + out_dir)\n",
    "            os.makedirs(out_dir)\n",
    "        else:\n",
    "            print(\"Overwriting previously saved prediction results in directory: \" + out_dir)\n",
    "            \n",
    "        for row in range(num_images):\n",
    "            idt = pred_indices[row]\n",
    "            Image.fromarray(dataset.images_outer2d[idt]).save(os.path.join(out_dir, 'images_outer2d_' + str(row) + '.jpg'))\n",
    "            #img.show()\n",
    "\n",
    "            Image.fromarray(pred[row]).save(os.path.join(out_dir, 'images_pred_' + str(row) + '.jpg'))\n",
    "            #img.show()\n",
    "\n",
    "            Image.fromarray(dataset.images_inner2d[idt]).save(os.path.join(out_dir, 'images_inner2d_' + str(row) + '.jpg'))\n",
    "            #img.show()\n",
    "\n",
    "            Image.fromarray(dataset.images[idt]).save(os.path.join(out_dir, 'fullimages_' + str(row) + '.jpg'))\n",
    "            #fullimg.show()\n",
    "\n",
    "            fullimg_pred = np.copy(dataset.images[idt])\n",
    "            center = (int(np.floor(fullimg_pred.shape[0] / 2.)), int(np.floor(fullimg_pred.shape[1] / 2.)))\n",
    "            fullimg_pred[center[0]-16:center[0]+16, center[1]-16:center[1]+16, :] = pred[row, :, :, :]\n",
    "            Image.fromarray(fullimg_pred).save(os.path.join(out_dir, 'fullimages_pred_' + str(row) + '.jpg'))\n",
    "            #img.show()\n",
    "\n",
    "def print_results_as_html(exp_name, pred, dataset, num_images=10):    \n",
    "    results_dir = os.path.join(\"predictions/\", \"assets/\", exp_name)\n",
    "    html_dir = os.path.join(\"predictions/\")\n",
    "    path_html = os.path.join(html_dir, \"results_\" + exp_name + \".html\")\n",
    "    print(\"Saving results as html to: \" + path_html)\n",
    "\n",
    "    with open(path_html, 'w') as fd:\n",
    "        fd.write(\"\"\"\n",
    "<table>\n",
    "  <tr>\n",
    "    <th style=\"width:132px\">Input</th>\n",
    "    <th style=\"width:68px\">Model prediction</th>\n",
    "    <th style=\"width:68px\">Correct output</th> \n",
    "    <th style=\"width:132px\">Input + prediction</th>\n",
    "    <th style=\"width:132px\">Input + correct output</th>\n",
    "  </tr>\n",
    "\"\"\")\n",
    "\n",
    "        for row in range(num_images):\n",
    "            fd.write(\"  <tr>\\n\")\n",
    "            fd.write('    <td><img src=\"%s/images_outer2d_%i.jpg\" width=\"128\" height=\"128\"></td>\\n' % (results_dir, row))\n",
    "            fd.write('    <td><img src=\"%s/images_pred_%i.jpg\" width=\"64\" height=\"64\"></td>\\n' % (results_dir, row))\n",
    "            fd.write('    <td><img src=\"%s/images_inner2d_%i.jpg\" width=\"64\" height=\"64\"></td>\\n' % (results_dir, row))\n",
    "            fd.write('    <td><img src=\"%s/fullimages_pred_%i.jpg\" width=\"128\" height=\"128\"></td>\\n' % (results_dir, row))\n",
    "            fd.write('    <td><img src=\"%s/fullimages_%i.jpg\" width=\"128\" height=\"128\"></td>\\n' % (results_dir, row))\n",
    "            fd.write('</tr>\\n')\n",
    "        \n",
    "        fd.write('</table>')\n",
    "\n",
    "def normalize_data(data):\n",
    "    data = data.astype('float32')\n",
    "    data /= 255\n",
    "    return data\n",
    "\n",
    "def denormalize_data(data):\n",
    "    data *= 255\n",
    "    data = data.astype('uint8')\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Define the main class for handling our dataset called InpaintingDataset\n",
    "\n",
    "class InpaintingDataset(object):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.images = []\n",
    "        self.images_outer2d = []\n",
    "        self.images_inner2d = []\n",
    "        self.images_outer_flat = []\n",
    "        self.images_inner_flat = []\n",
    "        self.captions_ids = []\n",
    "        self.captions_dict = []\n",
    "        self._is_dataset_loaded = False\n",
    "        self._is_flattened = False\n",
    "        self._is_normalized = False\n",
    "        self._num_rows = None\n",
    "    \n",
    "    def normalize(self):\n",
    "        if self._is_normalized:\n",
    "            print(\"WARNING: Attempting to normalize already normalized dataset... Ignoring this call...\")\n",
    "            return\n",
    "        self.images_outer_flat = normalize_data(self.images_outer_flat)\n",
    "        self.images_inner_flat = normalize_data(self.images_inner_flat)\n",
    "        self._is_normalized = True\n",
    "\n",
    "    def denormalize(self):\n",
    "        if not self._is_normalized:\n",
    "            print(\"WARNING: Attempting to denormalize already denormalized dataset... Ignoring this call...\")\n",
    "            return\n",
    "        self.images_outer_flat = denormalize_data(self.images_outer_flat)\n",
    "        self.images_inner_flat = denormalize_data(self.images_inner_flat)\n",
    "        self._is_normalized = False\n",
    "    \n",
    "    def load_jpgs_and_captions_and_flatten(self, paths_list, caption_path, force_reload = False):\n",
    "        with open(caption_path) as fd:\n",
    "            caption_dict = pkl.load(fd)\n",
    "        if not self._is_dataset_loaded and not force_reload:\n",
    "            images = []\n",
    "            images_outer2d = []\n",
    "            images_inner2d = []\n",
    "            images_outer_flat = []\n",
    "            images_inner_flat = []\n",
    "            captions_ids = []\n",
    "            captions_dict = []\n",
    "            for i, img_path in enumerate(paths_list):\n",
    "                img = Image.open(img_path)\n",
    "                img_array = np.array(img)\n",
    "\n",
    "                # File names look like this: COCO_train2014_000000520978.jpg\n",
    "                cap_id = os.path.basename(img_path)[:-4]\n",
    "\n",
    "                ### Get input/target from the images\n",
    "                center = (int(np.floor(img_array.shape[0] / 2.)), int(np.floor(img_array.shape[1] / 2.)))\n",
    "                if len(img_array.shape) == 3:\n",
    "                    image = np.copy(img_array)\n",
    "\n",
    "                    outer_2d = np.copy(img_array)\n",
    "                    outer_2d[center[0]-16:center[0]+16, center[1]-16:center[1]+16, :] = 0\n",
    "\n",
    "                    outer = np.copy(img_array)\n",
    "                    outer_mask = np.array(np.ones(np.shape(img_array)), dtype='bool')\n",
    "                    outer_mask[center[0]-16:center[0]+16, center[1]-16:center[1]+16, :] = False\n",
    "                    outer_flat = outer.flatten()\n",
    "                    outer_mask_flat = outer_mask.flatten()\n",
    "                    outer_flat = outer_flat[outer_mask_flat]\n",
    "\n",
    "                    inner2d = np.copy(img_array)\n",
    "                    inner2d = inner2d[center[0]-16:center[0]+16, center[1] - 16:center[1]+16, :]\n",
    "\n",
    "                    inner = np.copy(img_array)\n",
    "                    inner = inner[center[0]-16:center[0]+16, center[1] - 16:center[1]+16, :]\n",
    "                    inner_flat = inner.flatten()\n",
    "                else:\n",
    "                    # For now, ignore greyscale images\n",
    "                    continue\n",
    "                    #X_outer = np.copy(img_array)\n",
    "                    #X_outer[center[0]-16:center[0]+16, center[1]-16:center[1]+16] = 0\n",
    "                    #X_inner = img_array[center[0]-16:center[0]+16, center[1] - 16:center[1]+16]\n",
    "\n",
    "\n",
    "                #Image.fromarray(img_array).show()\n",
    "                images.append(image)\n",
    "                images_outer2d.append(outer_2d)\n",
    "                images_inner2d.append(inner2d)\n",
    "                images_outer_flat.append(outer_flat)\n",
    "                images_inner_flat.append(inner_flat)\n",
    "                captions_ids.append(cap_id)\n",
    "                captions_dict.append(caption_dict[cap_id])\n",
    "\n",
    "            self.images = np.array(images)\n",
    "            self.images_inner_flat = np.array(images_inner_flat)\n",
    "            self.images_outer_flat = np.array(images_outer_flat)\n",
    "            self.images_outer2d = np.array(images_outer2d)\n",
    "            self.images_inner2d = np.array(images_inner2d)\n",
    "            self.captions_ids = np.array(captions_ids)\n",
    "            self.captions_dict = np.array(captions_dict)\n",
    "\n",
    "            self._is_flattened = True\n",
    "            self._is_dataset_loaded = True\n",
    "            self._num_rows = self.images.shape[0]\n",
    "        else:\n",
    "            print(\"Dataset is already loaded. Skipping this call. Please pass the argument force_reload=True to force reloading of dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Create and initialize an empty InpaintingDataset object\n",
    "Dataset = InpaintingDataset(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from: datasets/mscoco_inpainting/inpainting/train2014/*.jpg\n",
      "Finished loading and pre-processing datasets...\n",
      "Summary of datasets:\n",
      "images.shape            = (82611, 64, 64, 3)\n",
      "images_outer2d.shape    = (82611, 64, 64, 3)\n",
      "images_inner2d.shape    = (82611, 32, 32, 3)\n",
      "images_outer_flat.shape = (82611, 9216)\n",
      "images_inner_flat.shape = (82611, 3072)\n",
      "captions_ids.shape      = (82611,)\n",
      "captions_dict.shape     = (82611,)\n"
     ]
    }
   ],
   "source": [
    "### Load training images and captions\n",
    "\n",
    "# Get captions dictionary path\n",
    "caption_path = os.path.join(mscoco, dict_key_captions)\n",
    "    \n",
    "# Get a list of all training images full filename paths\n",
    "data_path = os.path.join(path_mscoco, path_traindata)\n",
    "print(\"Loading images from: \" + data_path + \"/*.jpg\")\n",
    "train_images_paths = glob.glob(data_path + \"/*.jpg\")\n",
    "Dataset.load_jpgs_and_captions_and_flatten(train_images_paths, caption_path)\n",
    "\n",
    "print(\"Finished loading and pre-processing datasets...\")\n",
    "print(\"Summary of datasets:\")\n",
    "print(\"images.shape            = \" + str(Dataset.images.shape))\n",
    "print(\"images_outer2d.shape    = \" + str(Dataset.images_outer2d.shape))\n",
    "print(\"images_inner2d.shape    = \" + str(Dataset.images_inner2d.shape))\n",
    "print(\"images_outer_flat.shape = \" + str(Dataset.images_outer_flat.shape))\n",
    "print(\"images_inner_flat.shape = \" + str(Dataset.images_inner_flat.shape))\n",
    "print(\"captions_ids.shape      = \" + str(Dataset.captions_ids.shape))\n",
    "print(\"captions_dict.shape     = \" + str(Dataset.captions_dict.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing sanity check using first 10 elements of first 3 rows:\n",
      "[57 69 57 65 79 56 63 81 43 53]\n",
      "Row 0 passed sanity check!\n",
      "[197 202 195 167 164 147 104  87  57 102]\n",
      "Row 1 passed sanity check!\n",
      "[104 100  97  77  80  53 172 181 128 242]\n",
      "Row 2 passed sanity check!\n"
     ]
    }
   ],
   "source": [
    "### Sanity check:\n",
    "print(\"Performing sanity check using first 10 elements of first 3 rows:\")\n",
    "sanity_check_values = np.array([[57,   69,  57,  65,  79,  56,  63,  81,  43,  53],\n",
    "                                [197, 202, 195, 167, 164, 147, 104,  87,  57, 102],\n",
    "                                [104, 100,  97,  77,  80,  53, 172, 181, 128, 242]])\n",
    "for i in range(3):\n",
    "    top10 = Dataset.images_inner_flat[i, range(10)]\n",
    "    print(top10)\n",
    "    np.testing.assert_array_equal(top10, sanity_check_values[i])\n",
    "    print(\"Row \" + str(i) + \" passed sanity check!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into training and testing sets with shuffling...\n",
      "Splitting dataset into training and testing sets with shuffling...\n",
      "X_train.shape = (66088, 9216)\n",
      "X_test.shape  = (16523, 9216)\n",
      "Y_train.shape = (66088, 3072)\n",
      "Y_test.shape  = (16523, 3072)\n",
      "id_train.shape = (66088,)\n",
      "id_test.shape  = (16523,)\n"
     ]
    }
   ],
   "source": [
    "### Normalize datasets\n",
    "Dataset.normalize()\n",
    "\n",
    "### Split into training and testing data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "num_rows = Dataset.images.shape[0]\n",
    "indices = np.arange(num_rows)\n",
    "id_train, id_test = train_test_split(indices,\n",
    "                                     test_size=0.20,\n",
    "                                     random_state=1)\n",
    "\n",
    "### Generating the training and testing datasets (80%/20% train/test split)\n",
    "print(\"Splitting dataset into training and testing sets with shuffling...\")\n",
    "X_train, X_test, Y_train, Y_test = Dataset.images_outer_flat[id_train], \\\n",
    "                                   Dataset.images_outer_flat[id_test], \\\n",
    "                                   Dataset.images_inner_flat[id_train], \\\n",
    "                                   Dataset.images_inner_flat[id_test]\n",
    "\n",
    "print(\"Splitting dataset into training and testing sets with shuffling...\")\n",
    "print(\"X_train.shape = \" + str(X_train.shape))\n",
    "print(\"X_test.shape  = \" + str(X_test.shape))\n",
    "print(\"Y_train.shape = \" + str(Y_train.shape))\n",
    "print(\"Y_test.shape  = \" + str(Y_test.shape))\n",
    "print(\"id_train.shape = \" + str(id_train.shape))\n",
    "print(\"id_test.shape  = \" + str(id_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_train = [17442 21862  2835 ..., 50057  5192 77708]\n",
      "id_test  = [40977 36857 38411 ..., 77626 38615 14749]\n",
      "Y_train.shape = (66088, 3072)\n",
      "Y_train[0,1500:1550] = \n",
      "[ 0.48235294  0.45882353  0.46666667  0.38039216  0.37254903  0.42352942\n",
      "  0.41176471  0.40784314  0.47843137  0.43137255  0.43529412  0.51764709\n",
      "  0.3882353   0.40000001  0.47450981  0.51372552  0.52156866  0.58039218\n",
      "  0.53725493  0.53725493  0.57647061  0.29803923  0.28627452  0.3137255\n",
      "  0.40392157  0.38431373  0.40784314  0.45490196  0.41960785  0.43921569\n",
      "  0.33333334  0.28235295  0.30980393  0.33333334  0.28235295  0.31764707\n",
      "  0.38431373  0.40392157  0.5529412   0.32941177  0.34509805  0.48235294\n",
      "  0.35686275  0.37254903  0.47843137  0.33725491  0.33333334  0.39607844\n",
      "  0.33333334  0.32549021]\n"
     ]
    }
   ],
   "source": [
    "### Sanity check:\n",
    "print(\"id_train = \" + str(id_train))\n",
    "print(\"id_test  = \" + str(id_test))\n",
    "print(\"Y_train.shape = \" + str(Y_train.shape))\n",
    "print(\"Y_train[0,1500:1550] = \\n\" + str(Y_train[0,1500:1550]))\n",
    "\n",
    "idx = id_train[0]\n",
    "img = Image.fromarray(Dataset.images[0])\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "is_model_trained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MLP model...\n",
      "Model summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_64 (Dense)             (None, 512)               4719104   \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 3072)              1575936   \n",
      "=================================================================\n",
      "Total params: 6,557,696.0\n",
      "Trainable params: 6,557,696.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Compiling model...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'class_name' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-332-f63730e4f29e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Compiling model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0madam_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Default lr = 0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/paradiph/anaconda2/envs/keras/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, sample_weight_mode, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m                            \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m                            \u001b[0msample_weight_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m                            **kwargs)\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/paradiph/anaconda2/envs/keras/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m             \u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss_function\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/paradiph/anaconda2/envs/keras/lib/python2.7/site-packages/keras/losses.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0midentifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/paradiph/anaconda2/envs/keras/lib/python2.7/site-packages/keras/losses.pyc\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(name, custom_objects)\u001b[0m\n\u001b[1;32m     80\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                                     printable_module_name='loss function')\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/paradiph/anaconda2/envs/keras/lib/python2.7/site-packages/keras/utils/generic_utils.pyc\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 raise ValueError('Unknown ' + printable_module_name,\n\u001b[0;32m--> 157\u001b[0;31m                                  ':' + class_name)\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'class_name' referenced before assignment"
     ]
    }
   ],
   "source": [
    "if not is_model_trained:\n",
    "    print(\"Creating MLP model...\")\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=512, input_shape=(input_dim, )))\n",
    "    model.add(Activation('relu'))\n",
    "    if use_dropout:\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=512))\n",
    "    if use_sigmoid_final_layer:\n",
    "        model.add(Activation('sigmoid'))\n",
    "    else:\n",
    "        model.add(Activation('relu'))\n",
    "    if use_dropout:\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=output_dim))\n",
    "\n",
    "    # Print model summary\n",
    "    print(\"Model summary:\")\n",
    "    print(model.summary())\n",
    "\n",
    "    # Compile model\n",
    "    print(\"Compiling model...\")\n",
    "    adam_optimizer = optimizers.Adam(lr=0.0005) # Default lr = 0.001\n",
    "    model.compile(loss=loss_function, optimizer=adam_optimizer, metrics=[loss_function])\n",
    "\n",
    "    # Fit the model\n",
    "    print(\"Fitting model...\")\n",
    "    model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=num_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # evaluate the model\n",
    "    print(\"Evaluating model...\")\n",
    "    scores = model.evaluate(X_train, Y_train, batch_size=batch_size)\n",
    "    print(\"Training score %s: %.2f\" % (model.metrics_names[1], scores[1]))\n",
    "    scores = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "    print(\"Testing score %s: %.2f\" % (model.metrics_names[1], scores[1]))\n",
    "    is_model_trained = True\n",
    "\n",
    "    #%% Save model\n",
    "    save_model_info(experiment_name, model)\n",
    "else:\n",
    "    model_path = os.path.join('models/', experiment_name + '.h5')\n",
    "    print(\"Model was already trained, instead loading: \" + model_path)\n",
    "    model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_model_info(experiment_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Produce predictions\n",
    "Y_test_pred = model.predict(X_test, batch_size=batch_size)\n",
    "\n",
    "# Reshape predictions to a 2d image and denormalize data\n",
    "Y_test_pred = denormalize_data(Y_test_pred)\n",
    "num_rows = Y_test_pred.shape[0]\n",
    "Y_test_pred_2d = np.reshape(Y_test_pred, (num_rows, 32, 32, 3))\n",
    "\n",
    "# Denormalize all datasets\n",
    "Dataset.denormalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Save predictions to disk\n",
    "save_predictions_info(experiment_name, Y_test_pred_2d, id_test, Dataset, num_images=50)\n",
    "print_results_as_html(experiment_name, Y_test_pred_2d, Dataset, num_images=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(Y_test_pred_2d[0])\n",
    "print(Dataset.images_outer2d[id_test[0]])\n",
    "print(Dataset.images_inner2d[id_test[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Image.fromarray(Y_test_pred_2d[0]).save(\"a.jpg\")\n",
    "Image.fromarray(Y_test_pred_2d[0]).save(\"a.bmp\")\n",
    "Image.fromarray(Y_test_pred_2d[1]).save(\"aa.jpg\")\n",
    "Image.fromarray(Y_test_pred_2d[1]).save(\"aa.bmp\")\n",
    "Image.fromarray(Dataset.images_outer2d[id_test[0]]).save(\"b.jpg\")\n",
    "Image.fromarray(Dataset.images_inner2d[id_test[0]]).save(\"c.jpg\")\n",
    "Image.fromarray(Dataset.images[id_test[0]]).save(\"d.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
